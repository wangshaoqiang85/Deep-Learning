# Deep-Learning
深度学习算法笔记

## CNN（卷积神经网络）

1、卷积神经网络主要用在图像分类和物品识别 <br>
2、主要层次结构：数据输入层、卷积计算层、relu激励层、池化层、全连接层
* 数据输入层 <br>
数据输入层主要进行数据的预处理（白化），使学习算法的输入具有以下性质：特征之间的相关性较低;所有特征具有相同的方差 <br>
常见的三种数据预处理方式<br>
  * 去均值 ：将输入数据的各个维度中心化到0 
  * 归一化 ：将输入数据的各个维度的幅度归一化到同样的范围 
  * PCA/白化 ： 用PCA降维，白化是对数据的每个特征轴上的幅度归一化 
* 卷积计算层  <br>
演示链接： `http://cs231n.github.io/assets/conv-demo/index.html` 
  * 局部关联：局部数据识别 
  * 窗口滑动：滑动预先设定步长，移动位置来得到下一个窗口 
  * 深度：转换次数（结果产生的depth） 
  * 步长：设定每一步移动多少  
  * 填充值：可以在矩阵的周边添加一些扩充值（目的是解决图片输入不规整） 
  * 参数共享机制：假设每个神经元连接数据窗的权重是固定的。固定每个神经元的连接权重，可以将神经元看成一个模板；也就是每个神经元只关注一个特性，这样一来，需要计算的权重个数会大大的减少。
  * 卷积：把一组固定的权重和不同窗口内数据做内积  
* relu激励层<br>
使用激励函数来完成非线性的映射:
  * 双S和S函数用于全连接层
  * relu函数用于卷积计算层（迭代较快，但可能效果不佳）
  * 普遍使用elu函数
  * maxout函数：使用最大值来设置值
* 池化层<br> 
最大池化，平均池化
* 全连接层 <br>
对于数据的汇总计算
## RNN（循环神经网络）
1、BP神经网络和CNN的输入输出都是相互独立的；但是在实际应用中有些场景输出内容和之前的内容是有关联的
RNN引入记忆的概念；循环指其每一个元素执行相同任务，但是输出依赖于输入和记忆<br>
2、应用场景：自然语言处理（语言模型与文本分析）、机器翻译、语音识别、图像描述生成<br>


